from itertools import count
from os import link
from urllib import response
from bs4 import BeautifulSoup
import requests 
import requests.exceptions
import urllib.parse
from collections import deque
import re
from urllib.parse import urlsplit
import csv




user_url = "url"
urls = deque([user_url])

scraped_urls = set()
emails = set()
count = 0
try:
    while len(urls):
        count += 1
        if count == 40:
            break
        url = urls.popleft()
        scraped_urls.add(url)
        
        parts = urlsplit(url)
        base_url = "{0.netloc}".format(parts)
        
        path = url[:url.rfind('/')+1] if '/' in parts.path else url
        
        print('[%d] Processing %s' % (count, url))
        try:
            response = requests.get(url)
        except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError):   
            continue 
        
        new_emails = set(re.findall(r'[\w\.-]+@[\w\.-]+[fr]', response.text, re.I))
        emails.update(new_emails)
        print(emails) 
        
        
        soup = BeautifulSoup(response.text, features="lxml")
        
        for anchor in soup.find_all("a"):
            link = anchor.attrs['href'] if 'href' in anchor.attrs else''
            if link.startswith('/'):
                link = base_url +  link
            elif not link.startswith('http'):
                link = path + link 
            if not link in urls and not link in scraped_urls:
                urls.append(link) 
                
    
                  
except KeyboardInterrupt:
    print('[-] Colsing!')
    
    import csv

# ouverture en écriture (w, première lettre de write) d'un fichier
with open('essai.csv', 'w', newline='') as fichier:
    rows = emails
    # on déclare un objet writer 
    ecrivain = csv.writer(fichier)

    # écrire une ligne dans le fichier:
    ecrivain.writerow([emails])
    # quelques lignes:
    ecrivain.writerows(rows)
